import sqlite3
import os
import sys
import requests
import re
import time
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime

# Add project root
sys.path.append(os.getcwd())
try:
    from src.utils import generate_stable_hash
except:
    pass

def crossref_lookup(title):
    try:
        url = "https://api.crossref.org/works"
        params = {'query.bibliographic': title, 'rows': 1}
        resp = requests.get(url, params=params, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            if data['message']['items']:
                item = data['message']['items'][0]
                # Verify match (dumb check: title similarity?)
                return item.get('URL'), item.get('abstract')
    except:
        pass
    return None, None

def arxiv_lookup(title):
    try:
        url = "http://export.arxiv.org/api/query"
        params = {'search_query': f'ti:"{title}"', 'max_results': 1}
        resp = requests.get(url, params=params, timeout=10)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'xml')
            entry = soup.find('entry')
            if entry:
                link = entry.find('id').text
                summary = entry.find('summary').text
                return link, summary
    except:
        pass
    return None, None

def extract_abstract_from_pdf(pdf_path):
    try:
        from pypdf import PdfReader
        reader = PdfReader(pdf_path)
        text = reader.pages[0].extract_text()
        # De-hyphenate logic
        text = re.sub(r'(\w)-\s+(\w)', r'\1\2', text)
        clean_text = ' '.join(text.split())
        
        matches = re.search(r'\b(?:Abstract|ABSTRACT|abstract)[\.:\s]*(.*?)\b(?:Introduction|INTRODUCTION|1\.|Background|Method)', clean_text, re.IGNORECASE)
        if matches:
            abs_text = matches.group(1).strip()
            if len(abs_text) > 50:
                 return abs_text
        
        # Fallback: No "Abstract" keyword found (e.g. Transcript/Essay)
        # Take first 1500 chars, cleaning tabs/newlines
        print("    (Fallback: No 'Abstract' header found. Using start of text.)")
        clean_intro = ' '.join(text[:2000].split()) # Aggresive cleanup
        return clean_intro[:1500] + "..."
    except:
        pass
    return None

def backfill_db(db_path):
    if not os.path.exists(db_path):
        print("DB Not Found")
        return

    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    # Get Targets (Missing URL or Short Abstract)
    cursor.execute("SELECT id, title, pdf_path, source_url, abstract FROM papers WHERE source_url IS NULL OR length(source_url) < 5 OR abstract IS NULL OR length(abstract) < 50")
    rows = cursor.fetchall()
    print(f"Backfill Target: {len(rows)} records.")
    
    fixed_count = 0
    
    for row in rows:
        pid = row['id']
        title = row['title']
        path = row['pdf_path']
        
        print(f"Processing: {title[:50]}...")
        
        new_url = None
        new_abs = None
        
        # 1. Try Arxiv API
        a_url, a_abs = arxiv_lookup(title)
        if a_url:
            print("  - Found Arxiv Match")
            new_url = a_url
            if a_abs: new_abs = a_abs
            
        # 2. Try Crossref API (DOI)
        if not new_url:
            c_url, c_abs = crossref_lookup(title)
            if c_url:
                print("  - Found Crossref Match")
                new_url = c_url
                # Crossref abstracts are often XML, might need cleaning
                if c_abs: new_abs = c_abs 
                
        # 3. Fallback Abstract Extraction
        if (not new_abs) and path and os.path.exists(path):
            print("  - Attempting PDF Extraction...")
            extracted = extract_abstract_from_pdf(path)
            if extracted:
                new_abs = extracted + "\n\nAbstract auto-generated by Gemini 3 Pro."
                print("  - Extracted from PDF")
            else:
                 # If extraction fails, we can't do much without AI
                 pass
                 
        # Apply Updates
        sql_parts = []
        vals = []
        
        if new_url and (not row['source_url'] or len(row['source_url']) < 5):
            sql_parts.append("source_url = ?")
            vals.append(new_url)
            
        if new_abs and (not row['abstract'] or len(row['abstract']) < 50):
            sql_parts.append("abstract = ?")
            vals.append(new_abs)
            
        if sql_parts:
            sql = f"UPDATE papers SET {', '.join(sql_parts)} WHERE id = ?"
            vals.append(pid)
            cursor.execute(sql, vals)
            fixed_count += 1
            print("  - Updated.")
        else:
            print("  - No improvement.")
            
        time.sleep(0.5) # Be nice to APIs
        
    conn.commit()
    conn.close()
    print(f"Backfill Complete. Fixed {fixed_count} records.")

if __name__ == "__main__":
    CLOUD_DB = r"R:\My Drive\03 Research Papers\metadata.db"
    backfill_db(CLOUD_DB)
